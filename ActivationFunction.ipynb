{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632de42e-8ab5-44bd-a0f5-1cdd698d6eed",
   "metadata": {},
   "source": [
    "<h1>ðŸ”Œ Activation Functions in Neural Networks</h1>\n",
    "\n",
    "<p>\n",
    "Activation functions play a crucial role in neural networks. They introduce non-linearity into the model, allowing it to learn complex patterns beyond simple linear relationships.\n",
    "</p>\n",
    "\n",
    "<h2>ðŸ“š Common Activation Functions</h2>\n",
    "\n",
    "<h3>1. Sigmoid Function</h3>\n",
    "<p>\n",
    "The sigmoid function transforms input values ithnto a range between 0 and 1. It is also simply known as the <strong>sigmoid</strong> function.\n",
    "<br>\n",
    "<b>Usage:</b> Most commonly used in the <em>output layer</em> for binary classification tasks.\n",
    "</p>\n",
    "\n",
    "<h3>2. Tanh (Hyperbolic Tangent) Function</h3>\n",
    "<p>\n",
    "The tanh function maps input values to a range between -1 and 1. It generally performs better than sigmoid in hidden layers.\n",
    "<br>\n",
    "<b>Usage:</b> Recommended for use in <em>hidden layers</em> instead of sigmoid.\n",
    "</p>\n",
    "\n",
    "<h3>3. ReLU (Rectified Linear Unit)</h3>\n",
    "<p>\n",
    "ReLU outputs 0 for negative inputs and the input itself for positive values. It helps mitigate the vanishing gradient problem.\n",
    "<br>\n",
    "<b>Usage:</b> Use ReLU as the <em>default activation function</em> in hidden layers when unsure.\n",
    "</p>\n",
    "\n",
    "<h3>4. Leaky ReLU</h3>\n",
    "<p>\n",
    "Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient when the input is negative. It helps to avoid the \"dying ReLU\" problem.\n",
    "</p>\n",
    "\n",
    "<h3>5. Step Function</h3>\n",
    "<p>\n",
    "The step function outputs either 0 or 1 based on a threshold. It is one of the earliest activation functions but is rarely used in modern deep learning due to its lack of a gradient.\n",
    "</p>\n",
    "\n",
    "<h2>âœ… Practical Tips</h2>\n",
    "\n",
    "<ul>\n",
    "  <li>Use <strong>sigmoid</strong> in the <em>output layer</em> for binary classification.</li>\n",
    "  <li>Use <strong>tanh</strong> in <em>hidden layers</em> when better performance is needed over sigmoid.</li>\n",
    "  <li>Use <strong>ReLU</strong> as the <em>default choice</em> for hidden layers if unsure which to use.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3159a-046c-47ae-a976-29f52dd8f9cc",
   "metadata": {},
   "source": [
    "<h3>1. Sigmoid Function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6488ae0e-1634-459d-b954-71a0a4eda104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):             # This function will give output between 0 ans 1. \n",
    "    return 1 / (1 +math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a9a0f9-bf7b-4224-ba67-7cded4c60a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec9ef96-b932-4275-87e5-8d0055251134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024726231566347743"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b617698-e5e6-4b6f-82a9-dde7d0ff75a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975273768433653"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7c6c1-f143-41bd-8052-c9863d1f2c88",
   "metadata": {},
   "source": [
    "<h3>2. Tanh (Hyperbolic Tangent) Function</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba3dd68c-4dea-4bd3-bf63-d083561a670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):                          # This function will give output between -1 and 1.\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "544bd8cc-7ce9-4037-8f48-8ec4b83858b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8532a4e6-f270-4e1e-97cb-fda01cb2d10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tanh(-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd546831-d8ee-42db-8ae9-90b910222eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950547536867306"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3a657-ba77-4d6c-83a0-97a197b92407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
